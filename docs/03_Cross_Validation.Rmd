---
title: "Cross Validation"
author: "Javier Cara"
date: "Curso 2018-19"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El problema es que el modelo se ajusta muy bien a los datos que hemos utilizado para estimarlo, luego es razonable pensar que va a predecir bien dichos datos. Pero en la práctica, queremos el modelo para predecir datos que no conocemos. El objetivo es analizar la predicción del modelo frente a datos no conocidos.


# El método del subconjunto de validación (validation set approach)

- Dividimos en dos partes los datos (50%-50%, 40%-60%,...).
- En el **training set** estimamos el modelo.
- En el **test set** (**validation set**) predecimos la respuesta y calculamos el Mean Squared Error (MSE) entre la variable respuesta observada y la predicha.

$$
MSE = \frac{1}{n}\sum _{i=1}^{n}{(y_i - \hat y_i)^2}
$$

```{r}
datos = read.csv("Advertising.csv")
str(datos)
```

- Dividimos los datos en training set y test set:

```{r}
n = nrow(datos)
n_train = 100
n_test = n - n_train
```

```{r}
set.seed(115)

v = 1:n
pos_train = sample(v, n_train, replace = F) # muestreo sin reemplazamiento
pos_test = v[-pos_train]

# dividimos los datos en training set y validation set
datos_train = datos[pos_train,]
datos_test = datos[pos_test,]

plot(datos_train$TV, datos_train$sales, pch = 19, col = "blue")
points(datos_test$TV, datos_test$sales, pch = 19, col = "red")
legend(x=0,y=25, legend=c("train","test"), fill=c("blue","red"))
```

- En el training set estimamos el modelo:

```{r}
m1 = lm(sales ~ TV, data = datos_train)

# error en el training set
y_train = datos_train$sales
y_train_p = predict(m1,datos_train)
```

- Error en el validation set calculamos el MSE:

```{r}
source("MSE.R")
MSE # se muestra la definición de la funcion
```

```{r}
y_test = datos_test$sales
y_test_p = predict(m1,datos_test)

(mse_test = MSE(y_test,y_test_p))
```


Problemas:
- El MSE cambia en función de como se elija el training set y el validation set.
- Sólo se incluye una parte de los datos para estimar el modelo, por lo que la estimación es peor que incluyéndolos a todos.


# Leave-One-Out Cross-Validation

- Se dividen los datos en dos partes
    - training set: $\{(y_2,x_2), (y_3,x_3), \ldots, (y_n,x_n) \}$. Aquí se estima el modelo
    - test set: $(y_1,x_1)$. Calculamos el error de predicción

$$
MSE_1 = (y_1 - \hat y_1)^2
$$
- Repetimos el proceso con:
    - training set: $\{(y_1,x_1), (y_3,x_3), \ldots, (y_n,x_n) \}$. Aquí se estima el modelo
    - test set: $(y_2,x_2)$. Calculamos el error de predicción

$$
MSE_2 = (y_2 - \hat y_2)^2
$$
- Repetimos el proceso para calcular $MSE_3, MSE_4, \ldots, MSE_n$.

- el error final es la media

$$
CV = \frac{1}{n}\sum _{i=1}^{n}{MSE_i}
$$

```{r}
error_i = rep(0,n)
for (i in 1:n){
  datos_train = datos[-i,]
  datos_test = datos[i,]
  mi = lm(sales ~ TV, data = datos_train)
  
  yi = datos_test$sales
  yi_p = predict(mi,datos_test)
  
  error_i[i] = MSE(yi,yi_p)
}
(error_i_media = mean(error_i))
```

```{r}
plot(error_i)
lines(error_i_media*rep(1,n),col='red')
```


# k-fold Cross-Validation

Se divide aleatoriamente los datos en **k** partes (o folds). Cada parte tiene n1 = n/k datos.

- Para  i = 1:k
    - la parte i constituye el test set.
    - las otras partes constituyen el train set.
    - se calcula el MSE(i)
- El error total es:

$$
MSE_{TOTAL} = \frac{1}{k} \sum _{i=1}^k MSE_i
$$


```{r}
# numero de folds
num_folds = 5
n = nrow(datos)
n1 = trunc(n/num_folds)

# se definen las posiciones de test de cada fold
set.seed(342)
v = sample(1:n,n,replace = F) # vector auxiliar
pos = list()
for (k in 1:(num_folds-1)){
  pos[[k]] = v[((k-1)*n1+1):(k*n1)]
}
# el ultimo fold puede tener un numero diferente de datos
# (por redondeo)
pos[[num_folds]] = v[(n-n1+1):n]

v = 1:n
error_k = rep(0,num_folds)
for (k in 1:num_folds){
  # datos de training y de validation de cada fold
  pos_test = pos[[k]]
  pos_train = v[-pos_test]
  datos_train = datos[pos_train,]
  datos_test = datos[pos_test,]
  
  # estimamos el modelo
  mk = lm(sales ~ TV, data = datos_train)
  
  # error de prediccion
  y_va = datos_test$sales
  y_va_p = predict(mk,datos_test)
  
  error_k[k] = MSE(y_va,y_va_p)
}
(errork_media = mean(error_k))
```

## Funciones para el cross-validation

Utilizando las definiciones anteriores, se van a definir funciones para poder aplicar cross-validation en temas posteriores:

- funcion que calcula las posiciones de train y test dado el numero de folds ([descargar](cross_val_pos.R)):

```{r}
source("cross_val_pos.R")
cross_val_pos # se muestra el contenido de la función
```

Aplicamos estas funciones a los datos para comprobar que funcionan correctamente:

```{r}
set.seed(342)
num_datos = 10
num_folds = 3
cross_val_pos(num_datos,num_folds)
```





